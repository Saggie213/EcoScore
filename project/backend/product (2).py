# -*- coding: utf-8 -*-
"""Product.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13VBTFSQMKrTZUS76snkQvCGVY93zPVDF
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load datasets
df1 = pd.read_csv('sustainable_fashion_trends_2024.csv')
df2 = pd.read_csv('amazon_eco-friendly_products.csv')

# Clean Amazon dataset
df2 = df2.dropna(subset=['rating', 'reviewsCount', 'price'])
df2['price'] = df2['price'].replace('[\$,]', '', regex=True).astype(float)

# Create target variable
df2['purchased'] = ((df2['rating'] >= 3.5) & (df2['reviewsCount'] > 50)).astype(int)

# Merge with sustainability data
df = pd.merge(df2, df1, how='left', left_on='brand', right_on='Brand_Name')

# Fill missing sustainability features
df.fillna({
    'Sustainability_Rating': 'Unknown',
    'Carbon_Footprint_MT': df1['Carbon_Footprint_MT'].mean(),
    'Water_Usage_Liters': df1['Water_Usage_Liters'].mean(),
    'Waste_Production_KG': df1['Waste_Production_KG'].mean(),
    'Average_Price_USD': df1['Average_Price_USD'].mean()
}, inplace=True)

# Select features
features = ['category', 'material', 'brand', 'price', 'rating', 'reviewsCount',
            'Carbon_Footprint_MT', 'Water_Usage_Liters', 'Waste_Production_KG', 'Average_Price_USD']
target = 'purchased'

df = df[features + [target]].dropna()

# Encode categoricals
for col in ['category', 'material', 'brand']:
    df[col] = LabelEncoder().fit_transform(df[col].astype(str))

# Prepare feature matrix and target vector
X = df[features]
y = df[target]

# Split and scale
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, mean_squared_error

# Hyperparameter grid
param_grid = {
    'n_estimators': [100, 150],
    'learning_rate': [0.05, 0.1],
    'max_depth': [3, 5]
}

# Grid search
gb = GradientBoostingClassifier(random_state=42)
grid_search = GridSearchCV(estimator=gb, param_grid=param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train_scaled, y_train)

# Best model
best_model = grid_search.best_estimator_

import matplotlib.pyplot as plt
import seaborn as sns

# Evaluate
y_pred = best_model.predict(X_test_scaled)
y_proba = best_model.predict_proba(X_test_scaled)[:, 1]
acc = accuracy_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Accuracy: {acc:.4f}, RMSE: {rmse:.4f}")

# Feature importance
importances = best_model.feature_importances_
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(data=importance_df, x='Importance', y='Feature')
plt.title('Feature Importance - Tuned Gradient Boosting')
plt.tight_layout()
plt.savefig('feature_importance_plot.png')
plt.show()

# Attach prediction scores
X_test_df = X_test.copy()
X_test_df['purchase_likelihood'] = y_proba
X_test_df['actual_purchase'] = y_test.values

# Top 10 recommendations
top_products = X_test_df.sort_values(by='purchase_likelihood', ascending=False).head(10)
print(top_products)

import joblib

joblib.dump(best_model, 'gradient_boost_recommender_tuned.pkl')
joblib.dump(scaler, 'scaler_tuned.pkl')